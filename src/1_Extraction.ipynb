{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:41:56.410832Z",
     "end_time": "2023-04-24T14:41:57.140990Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'text': ['Experienced', 'staff', 'and', 'had', 'a', 'memorable', 'stay'], 'label': ['O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "{'text': ['India', 'as', 'a', 'country', 'has', 'always', 'fascinated', 'me', 'and', 'all', 'of', 'my', 'friends', 'who', 'have', 'been', 'there', 'always', 'have', 'wonderful', 'things', 'to', 'say', 'about', 'it', '.'], 'label': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "{'text': ['One', 'of', 'my', 'friends', 'who', 'had', 'been', 'there', 'before', 'was', 'planning', 'a', 'weeklong', 'trip', 'to', 'Rajasthan', 'in', 'India', 'and', 'I', 'decided', 'to', 'join', 'him', 'this', 'time', '.'], 'label': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O']}\n",
      "{'text': ['Yes', ',', 'it', 'really', 'was', 'a', 'great', 'experience', 'and', 'we', 'visited', 'various', 'places', 'but', 'the', 'most', 'wonderful', 'part', 'of', 'the', 'trip', 'was', 'our', 'stay', 'at', 'the', 'Oberoi', 'Udaivilas', 'Luxury', 'Hotel', '.'], 'label': ['O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'O', 'B-exp-Positive', 'I-exp-Positive', 'I-exp-Positive', 'I-exp-Positive', 'I-exp-Positive', 'I-exp-Positive', 'I-exp-Positive', 'O', 'O', 'O', 'O', 'B-targ-Positive', 'I-targ-Positive', 'I-targ-Positive', 'I-targ-Positive', 'I-targ-Positive', 'O']}\n",
      "{'text': ['I', 'can', 'â€™t', 'explain', 'in', 'words', 'how', 'grand', 'this', 'place', 'looks', '.'], 'label': ['O', 'O', 'O', 'O', 'O', 'O', 'B-exp-Positive', 'I-exp-Positive', 'B-targ-Positive', 'I-targ-Positive', 'B-exp-Positive', 'O']}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import paddle\n",
    "import paddle.nn.functional as F\n",
    "from paddlenlp.metrics import ChunkEvaluator\n",
    "from paddlenlp.datasets import load_dataset\n",
    "from paddlenlp.data import Pad, Stack, Tuple\n",
    "from paddlenlp.transformers import SkepTokenizer, SkepModel, LinearDecayWithWarmup\n",
    "import json\n",
    "import numpy as np\n",
    "import random\n",
    "\n",
    "train_path = \"./data/opener_en_extraction/train.json\"\n",
    "dev_path = \"./data/opener_en_extraction/dev.json\"\n",
    "test_path = \"./data/opener_en_extraction/test.json\"\n",
    "label_path = \"./data/opener_en_extraction/label.dict\"\n",
    "\n",
    "# load and process data\n",
    "\n",
    "def read_json(data_path):\n",
    "    with open(data_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        for sample in f:\n",
    "            sample = json.loads(sample)\n",
    "            sources = sample[\"sources\"]\n",
    "            targets = sample[\"targets\"]\n",
    "            expressions = sample[\"expressions\"]\n",
    "\n",
    "            label = []\n",
    "            for i in range(len(sources)):\n",
    "                source = sources[i]\n",
    "                target = targets[i]\n",
    "                expression = expressions[i]\n",
    "\n",
    "                if source != \"O\":\n",
    "                    label.append(source)\n",
    "                elif target != \"O\":\n",
    "                    label.append(target)\n",
    "                else:\n",
    "                    label.append(expression)\n",
    "\n",
    "            sample[\"label\"] = label\n",
    "\n",
    "            text = sample[\"text\"]\n",
    "            label = sample[\"label\"]\n",
    "            assert len(text) == len(label), f\"{text},  {label}\"\n",
    "            example = {\"text\": text, \"label\": label}\n",
    "\n",
    "            yield example\n",
    "\n",
    "def load_dict(dict_path):\n",
    "    with open(dict_path, \"r\", encoding=\"utf-8\") as f:\n",
    "        words = [word.strip() for word in f.readlines()]\n",
    "        word2id = dict(zip(words, range(len(words))))\n",
    "        id2word = dict((v, k) for k, v in word2id.items())\n",
    "\n",
    "        return word2id, id2word\n",
    "\n",
    "# load and process data\n",
    "label2id, id2label = load_dict(label_path)\n",
    "train_ds = load_dataset(read_json, data_path=train_path, lazy=False)\n",
    "dev_ds =  load_dataset(read_json, data_path=dev_path, lazy=False)\n",
    "test_ds =  load_dataset(read_json, data_path=test_path, lazy=False)\n",
    "\n",
    "for example in train_ds[:5]:\n",
    "    print(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2023-04-24 14:41:57,143] [    INFO]\u001B[0m - Found /home/christophe/.paddlenlp/models/skep_ernie_2.0_large_en/skep_ernie_2.0_large_en.vocab.txt\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input_ids:  [101, 100, 3095, 1998, 2018, 1037, 13432, 2994, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "seq_len:  9\n",
      "label:  [0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "input_ids:  [101, 100, 2004, 1037, 2406, 2038, 2467, 15677, 2033, 1998, 2035, 1997, 2026, 2814, 2040, 2031, 2042, 2045, 2467, 2031, 6919, 2477, 2000, 2360, 2055, 2009, 1012, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "seq_len:  28\n",
      "label:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "input_ids:  [101, 100, 1997, 2026, 2814, 2040, 2018, 2042, 2045, 2077, 2001, 4041, 1037, 100, 4440, 2000, 100, 1999, 100, 1998, 100, 2787, 2000, 3693, 2032, 2023, 2051, 1012, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "seq_len:  29\n",
      "label:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "\n",
      "input_ids:  [101, 100, 1010, 2009, 2428, 2001, 1037, 2307, 3325, 1998, 2057, 4716, 2536, 3182, 2021, 1996, 2087, 6919, 2112, 1997, 1996, 4440, 2001, 2256, 2994, 2012, 1996, 100, 100, 100, 100, 1012, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "seq_len:  33\n",
      "label:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0]\n",
      "\n",
      "input_ids:  [101, 100, 2064, 100, 4863, 1999, 2616, 2129, 2882, 2023, 2173, 3504, 1012, 102]\n",
      "token_type_ids:  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "seq_len:  14\n",
      "label:  [0, 0, 0, 0, 0, 0, 0, 1, 2, 3, 4, 1, 0, 0]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def convert_example_to_feature(example, tokenizer, label2id, max_seq_len=512, is_test=False):\n",
    "    encoded_inputs = tokenizer(example[\"text\"], is_split_into_words=True, max_seq_len=max_seq_len, return_length=True)\n",
    "\n",
    "    if not is_test:\n",
    "        # print(example[\"label\"])\n",
    "        label = [label2id[\"O\"]] + [label2id[label_term] for label_term in example[\"label\"]][:(max_seq_len - 2)] + [\n",
    "            label2id[\"O\"]]\n",
    "\n",
    "        assert len(encoded_inputs[\"input_ids\"]) == len(\n",
    "            label), f\"input_ids: {len(encoded_inputs['input_ids'])}, label: {len(label)}\"\n",
    "        return encoded_inputs[\"input_ids\"], encoded_inputs[\"token_type_ids\"], encoded_inputs[\"seq_len\"], label\n",
    "\n",
    "    return encoded_inputs[\"input_ids\"], encoded_inputs[\"token_type_ids\"], encoded_inputs[\"seq_len\"]\n",
    "\n",
    "model_name = \"skep_ernie_2.0_large_en\"\n",
    "batch_size = 8\n",
    "max_seq_len = 512\n",
    "\n",
    "tokenizer = SkepTokenizer.from_pretrained(model_name)\n",
    "trans_func = partial(convert_example_to_feature, tokenizer=tokenizer, label2id=label2id, max_seq_len=max_seq_len)\n",
    "train_ds = train_ds.map(trans_func, lazy=False)\n",
    "dev_ds = dev_ds.map(trans_func, lazy=False)\n",
    "test_ds = test_ds.map(trans_func, lazy=False)\n",
    "\n",
    "# print examples\n",
    "for example in train_ds[:5]:\n",
    "    print(\"input_ids: \", example[0])\n",
    "    print(\"token_type_ids: \", example[1])\n",
    "    print(\"seq_len: \", example[2])\n",
    "    print(\"label: \", example[3])\n",
    "    print()\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:41:57.143380Z",
     "end_time": "2023-04-24T14:41:57.225405Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2023-04-24 14:41:57,186] [    INFO]\u001B[0m - Already cached /home/christophe/.paddlenlp/models/skep_ernie_2.0_large_en/skep_ernie_2.0_large_en.pdparams\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use_gpu:  True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0424 14:41:57.187945 24113 gpu_resources.cc:61] Please NOTE: device: 0, GPU Compute Capability: 8.6, Driver API Version: 12.1, Runtime API Version: 11.7\n",
      "W0424 14:41:57.188593 24113 gpu_resources.cc:91] device: 0, cuDNN Version: 8.8.\n"
     ]
    }
   ],
   "source": [
    "batchify_fn = lambda samples, fn=Tuple(\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_id),\n",
    "    Pad(axis=0, pad_val=tokenizer.pad_token_type_id),\n",
    "    Stack(dtype=\"int64\"),\n",
    "    Pad(axis=0, pad_val= -1)\n",
    "): fn(samples)\n",
    "\n",
    "train_batch_sampler = paddle.io.BatchSampler(train_ds, batch_size=batch_size, shuffle=True)\n",
    "dev_batch_sampler = paddle.io.BatchSampler(dev_ds, batch_size=batch_size, shuffle=False)\n",
    "test_batch_sampler = paddle.io.BatchSampler(test_ds, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "train_loader = paddle.io.DataLoader(train_ds, batch_sampler=train_batch_sampler, collate_fn=batchify_fn)\n",
    "dev_loader = paddle.io.DataLoader(dev_ds, batch_sampler=dev_batch_sampler, collate_fn=batchify_fn)\n",
    "test_loader = paddle.io.DataLoader(test_ds, batch_sampler=test_batch_sampler, collate_fn=batchify_fn)\n",
    "\n",
    "class SkepForTokenClassification(paddle.nn.Layer):\n",
    "    def __init__(self, skep, num_classes=2, dropout=None):\n",
    "        super(SkepForTokenClassification, self).__init__()\n",
    "        self.num_classes = num_classes\n",
    "        self.skep = skep\n",
    "        self.dropout = paddle.nn.Dropout(dropout if dropout is not None else self.skep.config[\"hidden_dropout_prob\"])\n",
    "        self.classifier = paddle.nn.Linear(self.skep.config[\"hidden_size\"], num_classes)\n",
    "\n",
    "    def forward(self, input_ids, token_type_ids=None, position_ids=None, attention_mask=None):\n",
    "        sequence_output, _ = self.skep(input_ids, token_type_ids=token_type_ids, position_ids=position_ids, attention_mask=attention_mask)\n",
    "\n",
    "        sequence_output = self.dropout(sequence_output)\n",
    "        logits = self.classifier(sequence_output)\n",
    "        return logits\n",
    "\n",
    "def set_seed(seed):\n",
    "    paddle.seed(seed)\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "\n",
    "# model hyperparameter  setting\n",
    "num_epoch = 3\n",
    "learning_rate = 3e-5\n",
    "weight_decay = 0.01\n",
    "warmup_proportion = 0.1\n",
    "max_grad_norm = 1.0\n",
    "log_step = 20\n",
    "eval_step = 100\n",
    "seed = 1000\n",
    "checkpoint = \"./checkpoint/\"\n",
    "\n",
    "set_seed(seed)\n",
    "use_gpu = True if paddle.get_device().startswith(\"gpu\") else False\n",
    "print(\"use_gpu: \", use_gpu)\n",
    "if use_gpu:\n",
    "    paddle.set_device(\"gpu:0\")\n",
    "if not os.path.exists(checkpoint):\n",
    "    os.mkdir(checkpoint)\n",
    "\n",
    "skep = SkepModel.from_pretrained(model_name)\n",
    "model = SkepForTokenClassification(skep, num_classes=len(label2id))\n",
    "\n",
    "num_training_steps = len(train_loader) * num_epoch\n",
    "lr_scheduler = LinearDecayWithWarmup(learning_rate=learning_rate, total_steps=num_training_steps, warmup=warmup_proportion)\n",
    "decay_params = [p.name for n, p in model.named_parameters() if not any(nd in n for nd in [\"bias\", \"norm\"])]\n",
    "grad_clip = paddle.nn.ClipGradByGlobalNorm(max_grad_norm)\n",
    "optimizer = paddle.optimizer.AdamW(learning_rate=lr_scheduler, parameters=model.parameters(), weight_decay=weight_decay, apply_decay_param_fun=lambda x: x in decay_params, grad_clip=grad_clip)\n",
    "\n",
    "metric = ChunkEvaluator(label2id.keys())"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:41:57.186690Z",
     "end_time": "2023-04-24T14:41:58.606504Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 1 - global_step: 20/654 - loss:0.704549\n",
      "epoch: 1 - global_step: 40/654 - loss:0.529646\n",
      "epoch: 1 - global_step: 60/654 - loss:0.542912\n",
      "epoch: 1 - global_step: 80/654 - loss:0.646399\n",
      "epoch: 1 - global_step: 100/654 - loss:0.335785\n",
      "best F1 performence has been updated: 0.00000 --> 0.45697\n",
      "evalution result: precision: 0.42463, recall: 0.49464,  F1: 0.45697\n",
      "epoch: 1 - global_step: 120/654 - loss:0.375430\n",
      "epoch: 1 - global_step: 140/654 - loss:0.322531\n",
      "epoch: 1 - global_step: 160/654 - loss:0.336831\n",
      "epoch: 1 - global_step: 180/654 - loss:0.596456\n",
      "epoch: 1 - global_step: 200/654 - loss:0.372007\n",
      "best F1 performence has been updated: 0.45697 --> 0.55368\n",
      "evalution result: precision: 0.55782, recall: 0.54960,  F1: 0.55368\n",
      "epoch: 2 - global_step: 220/654 - loss:0.593220\n",
      "epoch: 2 - global_step: 240/654 - loss:0.203132\n",
      "epoch: 2 - global_step: 260/654 - loss:0.145418\n",
      "epoch: 2 - global_step: 280/654 - loss:0.129898\n",
      "epoch: 2 - global_step: 300/654 - loss:0.318746\n",
      "best F1 performence has been updated: 0.55368 --> 0.59365\n",
      "evalution result: precision: 0.55401, recall: 0.63941,  F1: 0.59365\n",
      "epoch: 2 - global_step: 320/654 - loss:0.197137\n",
      "epoch: 2 - global_step: 340/654 - loss:0.184375\n",
      "epoch: 2 - global_step: 360/654 - loss:0.219830\n",
      "epoch: 2 - global_step: 380/654 - loss:0.115675\n",
      "epoch: 2 - global_step: 400/654 - loss:0.383466\n",
      "best F1 performence has been updated: 0.59365 --> 0.61054\n",
      "evalution result: precision: 0.58642, recall: 0.63673,  F1: 0.61054\n",
      "epoch: 2 - global_step: 420/654 - loss:0.089525\n",
      "epoch: 3 - global_step: 440/654 - loss:0.158750\n",
      "epoch: 3 - global_step: 460/654 - loss:0.154295\n",
      "epoch: 3 - global_step: 480/654 - loss:0.179162\n",
      "epoch: 3 - global_step: 500/654 - loss:0.106408\n",
      "best F1 performence has been updated: 0.61054 --> 0.61857\n",
      "evalution result: precision: 0.58137, recall: 0.66086,  F1: 0.61857\n",
      "epoch: 3 - global_step: 520/654 - loss:0.115146\n",
      "epoch: 3 - global_step: 540/654 - loss:0.070061\n",
      "epoch: 3 - global_step: 560/654 - loss:0.077301\n",
      "epoch: 3 - global_step: 580/654 - loss:0.023569\n",
      "epoch: 3 - global_step: 600/654 - loss:0.083899\n",
      "best F1 performence has been updated: 0.61857 --> 0.62278\n",
      "evalution result: precision: 0.61190, recall: 0.63405,  F1: 0.62278\n",
      "epoch: 3 - global_step: 620/654 - loss:0.050648\n",
      "epoch: 3 - global_step: 640/654 - loss:0.158477\n",
      "evalution result: precision: 0.59429, recall: 0.64209,  F1: 0.61727\n"
     ]
    }
   ],
   "source": [
    "def evaluate(model, data_loader, metric):\n",
    "\n",
    "    model.eval()\n",
    "    metric.reset()\n",
    "    for idx, batch_data in enumerate(data_loader):\n",
    "        input_ids, token_type_ids, seq_lens, labels = batch_data\n",
    "        logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "\n",
    "        # count metric\n",
    "        predictions = logits.argmax(axis=2)\n",
    "        num_infer_chunks, num_label_chunks, num_correct_chunks = metric.compute(seq_lens, predictions, labels)\n",
    "        metric.update(num_infer_chunks.numpy(), num_label_chunks.numpy(), num_correct_chunks.numpy())\n",
    "\n",
    "    precision, recall, f1 = metric.accumulate()\n",
    "    return precision, recall, f1\n",
    "\n",
    "def train():\n",
    "    # start to train model\n",
    "    global_step, best_f1 = 1, 0.\n",
    "    model.train()\n",
    "    for epoch in range(1, num_epoch+1):\n",
    "        for batch_data in train_loader():\n",
    "            input_ids, token_type_ids, _, labels = batch_data\n",
    "            # logits: batch_size, seql_len, num_tags\n",
    "            logits = model(input_ids, token_type_ids=token_type_ids)\n",
    "            loss = F.cross_entropy(logits.reshape([-1, len(label2id)]), labels.reshape([-1]), ignore_index=-1)\n",
    "\n",
    "            loss.backward()\n",
    "            lr_scheduler.step()\n",
    "            optimizer.step()\n",
    "            optimizer.clear_grad()\n",
    "\n",
    "            if global_step > 0 and global_step % log_step == 0:\n",
    "                print(f\"epoch: {epoch} - global_step: {global_step}/{num_training_steps} - loss:{loss.numpy().item():.6f}\")\n",
    "            if (global_step > 0 and global_step % eval_step == 0) or global_step == num_training_steps:\n",
    "                precision, recall, f1  = evaluate(model, dev_loader,  metric)\n",
    "                model.train()\n",
    "                if f1 > best_f1:\n",
    "                    print(f\"best F1 performence has been updated: {best_f1:.5f} --> {f1:.5f}\")\n",
    "                    best_f1 = f1\n",
    "                    paddle.save(model.state_dict(), f\"{checkpoint}/best_ext.pdparams\")\n",
    "                print(f'evalution result: precision: {precision:.5f}, recall: {recall:.5f},  F1: {f1:.5f}')\n",
    "\n",
    "            global_step += 1\n",
    "\n",
    "    paddle.save(model.state_dict(), f\"{checkpoint}/final_ext.pdparams\")\n",
    "\n",
    "train()"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:41:58.611928Z",
     "end_time": "2023-04-24T14:42:48.832851Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001B[32m[2023-04-24 14:42:49,249] [    INFO]\u001B[0m - Already cached /home/christophe/.paddlenlp/models/skep_ernie_2.0_large_en/skep_ernie_2.0_large_en.pdparams\u001B[0m\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "evalution result: precision: 0.60681, recall: 0.63636,  F1: 0.62124\n"
     ]
    }
   ],
   "source": [
    "# load model\n",
    "model_path = \"./checkpoint/best_ext.pdparams\"\n",
    "\n",
    "loaded_state_dict = paddle.load(model_path)\n",
    "skep = SkepModel.from_pretrained(model_name)\n",
    "model = SkepForTokenClassification(skep, num_classes=len(label2id))\n",
    "model.load_dict(loaded_state_dict)\n",
    "\n",
    "# evalute on test data\n",
    "precision, recall, f1  = evaluate(model, test_loader,  metric)\n",
    "print(f'evalution result: precision: {precision:.5f}, recall: {recall:.5f},  F1: {f1:.5f}')\n"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-04-24T14:42:48.834710Z",
     "end_time": "2023-04-24T14:42:51.238316Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
